// AI æ¨¡å‹ç¼“å­˜ Service Worker
const CACHE_NAME = 'ai-models-v1';
const MODEL_CACHE_NAME = 'model-files-v1';
const RESPONSE_CACHE_NAME = 'ai-responses-v1';

// éœ€è¦ç¼“å­˜çš„æ¨¡å‹æ–‡ä»¶æ¨¡å¼
const MODEL_PATTERNS = [
  /\/models\//,
  /huggingface\.co/,
  /\.bin$/,
  /\.json$/,
  /\.safetensors$/,
  /tokenizer/,
  /config\.json$/,
  /pytorch_model/,
  /onnx/
];

// å®‰è£…äº‹ä»¶
self.addEventListener('install', (event) => {
  console.log('ğŸ”§ Service Worker æ­£åœ¨å®‰è£…...');
  self.skipWaiting();
});

// æ¿€æ´»äº‹ä»¶
self.addEventListener('activate', (event) => {
  console.log('âœ… Service Worker å·²æ¿€æ´»');
  event.waitUntil(
    Promise.all([
      self.clients.claim(),
      cleanupOldCaches()
    ])
  );
});

// æ‹¦æˆªç½‘ç»œè¯·æ±‚
self.addEventListener('fetch', (event) => {
  const url = new URL(event.request.url);
  
  // æ£€æŸ¥æ˜¯å¦æ˜¯æ¨¡å‹æ–‡ä»¶è¯·æ±‚
  const isModelFile = MODEL_PATTERNS.some(pattern => 
    pattern.test(url.pathname) || pattern.test(url.hostname)
  );
  
  if (isModelFile) {
    console.log('ğŸ¯ æ‹¦æˆªæ¨¡å‹æ–‡ä»¶è¯·æ±‚:', url.pathname);
    event.respondWith(handleModelRequest(event.request));
  }
});

// å¤„ç†æ¨¡å‹æ–‡ä»¶è¯·æ±‚
async function handleModelRequest(request) {
  const cache = await caches.open(MODEL_CACHE_NAME);
  const cachedResponse = await cache.match(request);
  
  if (cachedResponse) {
    console.log('âš¡ ä»ç¼“å­˜åŠ è½½æ¨¡å‹æ–‡ä»¶:', request.url);
    // æ·»åŠ ç¼“å­˜æ ‡è¯†å¤´
    const response = cachedResponse.clone();
    response.headers.set('X-Cache-Status', 'HIT');
    return response;
  }
  
  try {
    console.log('ğŸ“¥ ä¸‹è½½å¹¶ç¼“å­˜æ¨¡å‹æ–‡ä»¶:', request.url);
    
    // æ˜¾ç¤ºä¸‹è½½è¿›åº¦
    const response = await fetchWithProgress(request);
    
    // åªç¼“å­˜æˆåŠŸçš„å“åº”
    if (response.status === 200) {
      const responseClone = response.clone();
      await cache.put(request, responseClone);
      console.log('ğŸ’¾ æ¨¡å‹æ–‡ä»¶å·²ç¼“å­˜:', request.url);
    }
    
    return response;
  } catch (error) {
    console.error('âŒ æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥:', error);
    
    // è¿”å›ç¦»çº¿æç¤º
    return new Response(
      JSON.stringify({ error: 'æ¨¡å‹æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥' }),
      {
        status: 503,
        headers: { 'Content-Type': 'application/json' }
      }
    );
  }
}

// å¸¦è¿›åº¦çš„ä¸‹è½½
async function fetchWithProgress(request) {
  const response = await fetch(request);
  
  if (!response.body) {
    return response;
  }
  
  const contentLength = response.headers.get('Content-Length');
  if (!contentLength) {
    return response;
  }
  
  const total = parseInt(contentLength, 10);
  let loaded = 0;
  
  const stream = new ReadableStream({
    start(controller) {
      const reader = response.body.getReader();
      
      function pump() {
        return reader.read().then(({ done, value }) => {
          if (done) {
            controller.close();
            return;
          }
          
          loaded += value.length;
          const progress = Math.round((loaded / total) * 100);
          
          // å‘é€è¿›åº¦åˆ°ä¸»çº¿ç¨‹
          self.clients.matchAll().then(clients => {
            clients.forEach(client => {
              client.postMessage({
                type: 'DOWNLOAD_PROGRESS',
                url: request.url,
                progress,
                loaded,
                total
              });
            });
          });
          
          controller.enqueue(value);
          return pump();
        });
      }
      
      return pump();
    }
  });
  
  return new Response(stream, {
    headers: response.headers
  });
}

// æ¸…ç†æ—§ç¼“å­˜
async function cleanupOldCaches() {
  const cacheNames = await caches.keys();
  const oldCaches = cacheNames.filter(name => 
    name.startsWith('ai-models-') && name !== CACHE_NAME
  );
  
  await Promise.all(
    oldCaches.map(name => caches.delete(name))
  );
  
  console.log('ğŸ§¹ æ¸…ç†äº†', oldCaches.length, 'ä¸ªæ—§ç¼“å­˜');
}

// ç›‘å¬æ¶ˆæ¯
self.addEventListener('message', async (event) => {
  const { type, data } = event.data;
  
  switch (type) {
    case 'CLEAR_CACHE':
      await clearAllCaches();
      event.ports[0]?.postMessage({ success: true });
      break;
      
    case 'GET_CACHE_SIZE':
      const size = await getCacheSize();
      event.ports[0]?.postMessage({ size });
      break;
      
    case 'PRELOAD_MODEL':
      await preloadModel(data.modelUrl);
      event.ports[0]?.postMessage({ success: true });
      break;
      
    default:
      console.log('æœªçŸ¥æ¶ˆæ¯ç±»å‹:', type);
  }
});

// æ¸…ç†æ‰€æœ‰ç¼“å­˜
async function clearAllCaches() {
  const cacheNames = [MODEL_CACHE_NAME, RESPONSE_CACHE_NAME];
  
  for (const cacheName of cacheNames) {
    const cache = await caches.open(cacheName);
    const keys = await cache.keys();
    await Promise.all(keys.map(key => cache.delete(key)));
  }
  
  console.log('ğŸ—‘ï¸ æ‰€æœ‰ç¼“å­˜å·²æ¸…ç†');
}

// è·å–ç¼“å­˜å¤§å°
async function getCacheSize() {
  let totalSize = 0;
  const cacheNames = [MODEL_CACHE_NAME, RESPONSE_CACHE_NAME];
  
  for (const cacheName of cacheNames) {
    const cache = await caches.open(cacheName);
    const keys = await cache.keys();
    
    for (const key of keys) {
      const response = await cache.match(key);
      if (response) {
        const blob = await response.blob();
        totalSize += blob.size;
      }
    }
  }
  
  return totalSize;
}

// é¢„åŠ è½½æ¨¡å‹
async function preloadModel(modelUrl) {
  try {
    console.log('ğŸ”„ é¢„åŠ è½½æ¨¡å‹:', modelUrl);
    const request = new Request(modelUrl);
    await handleModelRequest(request);
    console.log('âœ… æ¨¡å‹é¢„åŠ è½½å®Œæˆ:', modelUrl);
  } catch (error) {
    console.error('âŒ æ¨¡å‹é¢„åŠ è½½å¤±è´¥:', error);
  }
}